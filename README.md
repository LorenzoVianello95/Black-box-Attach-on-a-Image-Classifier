Black-Box Attacks against Cifar10 dataset,
implementation of the model illustrated in the article "Practical Black-Box Attacks against Machine Learning" by Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik and Ananthram Swami.

1. INTRODUCTION:
“Machine learning models, e.g., deep neural networks(DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers.”
In these years it is possible to see a huge diffusion of machine learning techniques and the increase in the fields in which these technologies are applied.
As in all circumstances, in parallel with a legitimate use of these innovations, grows the field whose ultimate goal is to bypass and deceive them, with more or less noble purposes.
Like for example, malicious content like malware identified as legitimate, or controlling vehicle behavior.
For this reason it is important to know what are the developments in this field in order to prevent such deception.

2. BLACK BOX:
My goal in this project is exactly this: given a neural network of which there is no information about its internal structure (for this reason black box), create a dataset that is not recognized by the network.
About the network that we will try to deceive, henceforth we will call it Oracle, we need only know what are the domains in which it works:
we need to know if it processes images, text files, audio or anything else. Moreover, in order to reach our goal, it is necessary for us to have free access to the forecasts that the network can make.
In my case I chose as Oracle a cnn that classifies images belonging to the CIFAR10 dataset.

3. CIFAR10:
The CIFAR-10 dataset is a collection of images that contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class. 














My oracle is a cnn trained on the first 50000 images of the dataset. The accuracy thus obtained is around 70%.


4.ADVERSARIAL STRATEGY:
as already mentioned, my purpose is to create modified inputs in such a way that they are not recognized correctly by the network but at the same time that they appear unchanged to the human observer. To do this we want to produce a minimally alliterated version of any input x, named adversarial sample, and denoted x’, misclassified by oracle O: O(x)!= O(x’). Adversarial samples solve the following optimization problem:
x’= x + argmin{ k: O(x+k)!=O(x)} =x+dx
For examples :











The strategy is to learn a substitute for the target model using a synthetic dataset, generated by the adversary and labeled by observing the oracle output.  Then, adversarial examples are crafted using this substitute.  We expect the target DNN to misclassify them due to transferability between architectures.
This leads us to propose the following strategy:
1.Substitute Model Training:
the attacker queries the oracle with synthetic inputs selected by a Jacobian-based heuristic to build a model F approximating the oracle model O ’s decision boundaries.
2.Adversarial Sample Crafting:
the attacker uses substitute network F to craft adversarial samples, which are then misclassified by oracle O due to the transferability of adversarial samples.
  
4.1- SUBSTITUTE MODEL TRAINING:

To build our substitute we must at least have some partial knowledge of the oracle input (e.g. images, text) and expected output (e.g. classification). The adversary can thus use an architecture adapted to the input-output relation. For instance, a convolutional neural network is suitable for image classification.
For my task I chose to use a cnn built with tensorflow.  Its internal structure consists of the combination of some convolutional layers and some fully connected, there are also some Relu functions to reduce the linearity of the system.

Generating a Synthetic Dataset: The algorithm is used to generate synthetic training inputs, it is based on identifying directions in which the model’s output is
varying, around an initial set of training points. In this way we better approximate the decision boundaries.

















A critical step in this process is the Jacobian-based Dataset Augmentation (Step 8), at a theoretical level this step is equivalent to evaluating the sign of the Jacobian matrix dimension corresponding to the label assigned to input by the oracle. From an implementation point of view, in my program, it is equivalent to:


y_pred = tf.nn.softmax(layer_fc2)
jacobian=[tf.gradients(y_pred[:, c], x)[0] for c in range(0, num_classes)]

jac = session.run(jacobian, feed_dict=feed_dict_train)
lab = np.argmax(batch_y[0])
g = jac[lab]
el = batch_x +lmbda* np.sign(g)

I tried to do more tests on my substitute network, first changing the size of the initial datasets, then the training parameters (the number of epochs and lambda).
The experiments I have done use the test set of cifar 10, ie the last 10,000 images: these were not used during the training of the Oracle, but were used to calculate its accuracy.



These are some results obtained, with the relative considerations:
- As it is written in the reference paper the lambda value does not particularly affect the results, after some tests, and based on the recommended values, I chose lambda = 0.1.

Lambda:
Accuracy (after 6 epochs and with lambda=0.1) of the substitute over the Test set
0.3
22.20%
0.1
25.45
0.05
23.73%

-Also the number of rho epochs does not particularly affect the results, in fact after a certain number of iterations the accuracy and the loss tend to an asymptotic value. A good value for the rho parameter is around 5/6.
		
epochs
Training data=100
Training data=500
Training data=1000
1
12.30%
27.84%
33.31%
2
23.04%
29.63%
32.22%
3
25.56%
31.85%
34.52%
4
25.19%
30.86%
34.38%
5
25.23%
31.04%
34.41%
6
25.45%
31.05%
34.08%

-what particularly affects the results is the number of images of the initial dataset:

Size of the Initial Training Set 
Accuracy (after 6 epochs and with lambda=0.1) of the substitute over the Test set
100
25.45%
500
31.05%
1000
34.70%

 
We immediately observe that the accuracy reached is far below the current state of art on the CIFAR10 dataset.
But we have to keep in mind that the goal is not to reach a network that correctly classifies the cifar10 dataset, but rather a network that approximates as much as possible our Oracle.

4.2 ADVERSARIAL SAMPLE CRAFTING

Once the adversary trained a substitute DNN, it uses it to craft adversarial samples.
This is performed by implementing an approach called Goodfellow 
et al. Algorithm or fast gradient sign method.
This technique introduce some input modifications in order to select a small perturbation achieving the misclassification goal.
Its behavior is described on the right, and he plans to add a small perturbation proportional to the sign of the gradient of the cost function of the model.
The value of the input variation parameter ε factoring the sign matrix controls the perturbation’s amplitude. Increasing its value increases the likelihood of being misclassified by model F but on the contrary makes adversarial samples easier to detect by humans.

From an implementation point of view, in my program, it is equivalent to:

cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,
                                                        labels=y_true)
cost = tf.reduce_mean(cross_entropy)
grads_cost=tf.gradients(cost,x) …
grad_c = session.run(tf.sign(grads_cost), feed_dict=feed_dict)
new_batch = batch_x + epsilon * np.array(grad_c[0])
The perturbations created, based on the F network, can be transferred to the Oracle O, these are the results I obtained:

1. Varying substitute training size:

Size of the Initial Training Set 
Accuracy of the Oracle over the Modified Test set(epsilon=0.3)
100
22.93%
500
20.79%
1000
19.89%

2. Varying input variation parameter ε (keeping Size of the Initial Training Set =1000):

input variation parameter ε:
Accuracy of the Oracle over the Modified Test set:
0.3
19.89%
0.2
21.18%
0.1
23.25%
0.05
25.24%
0.03
28.77%
0.01
43.88%

Images visualization changing epsilon:

Original images and Oracle predictions: 				epsilon= 0.3



 













epsilon=0.1							epsilon=0.05



















epsilon =0.03						epsilon=0.01

















Confusion Matrices changing epsilon:

Confusion matrix Oracle with respect to initial test-set





			








epsilon=0.1

Perturbed data prediction 					Perturbed data prediction		
with respect to real values					with respect to original prediction


	









epsilon=0.05

Perturbed data prediction 					Perturbed data prediction		
with respect to real values					with respect to original prediction










epsilon=0.01

Perturbed data prediction 					Perturbed data prediction		
with respect to real values					with respect to original prediction
















It is evident from what is written above, and from the images, that parameters calibration is a very important step for our attack on a trained network.
In particular, by varying epsilon, one can choose to make the perturbations less evident while maintaining them effective.
In my opinion for this configuration an acceptable solution that meets both needs is epsilon = 0.03.

5- CONCLUSIONS:
We introduced an attack, based on a novel substitute training algorithm using synthetic data generation, to craft adversarial examples misclassified by black-box DNNs.
We did it on the CIFAR 10 dataset and we achieved good results. In fact we have created a perturbed dataset that is recognized by the Oracle with an accuracy of about 20% but, at the same time, the images do not show particularly visible perturbations. This work has allowed me to deepen a very important topic in many areas, first of all computer security.


6- REFERENCES:
git hub accounts of: Luyu Wang, wayne315315, Yonatan Geifman
article: http://analyticgradient.com/oracle-attacks-on-convnets.html
 	"Practical Black-Box Attacks against Machine Learning" by Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, 	Somesh Jha, Z. Berkay Celik and Ananthram Swami.
dataset: Canadian Institute for Advanced Research, CIFAR10







